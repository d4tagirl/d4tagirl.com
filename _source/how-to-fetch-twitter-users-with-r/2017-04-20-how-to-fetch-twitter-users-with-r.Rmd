---
layout: post
title:  How to fetch Twitter users with R
date: "2017-05-15 06:11:29 UYT"
published: true
tags: [rstats, r, Twitter, rtweet, purrr, map, ggmap]
description: How to fetch Twitter users and clean the data using R!
---
Here I show how to fetch Twitter users using the `rtweet` package, and clean the data using the `tidyverse` set of packages, for later usage in plotting animated maps.  

<!--more-->

This is the first one of a 3-posts-series, where I go from fetching Twitter users and preparing the data to visualizing it (If I wanted to show everything I've done in a single post, it would be almost as long as my first one! And believe me: nobody wants that `r emo::ji("stuck_out_tongue_closed_eyes")` ):

1. How to fetch Twitter users with R: this one, the title is kind of self explanatory...
2. [How to deal with ggplotly huge maps]({% post_url 2017-04-26-how-to-deal-with-ggplotly-huge-maps %}): where I go through the details of why I chose not to use `ggplotly` and use `plot_geo` instead to generate the HTML.
3. [How to plot animated maps with gganimate]({% post_url 2017-04-24-how-to-plot-animated-maps-with-gganimate %}): again, pretty obvious subject.
 
Finally [I present my favourite visualization here]({% post_url 2017-05-10-visualizing-r-ladies-growth %}).

I should warn you that there are a lot of emojis in this series, courtesy of the [`emo` package Hadley recently released](https://github.com/hadley/emo) and I fanatically adopted `r emo::ji("innocent")`

Let's get started!

## Getting Twitter users

I had to learn how to retrieve data from the Twitter API, and I chose to use the `rtweet` package, which is super easy to use! Since I only use public data I don't have to worry about getting my Twitter personal access token. 

Every R-Ladies' chapter uses a standard handle, with the *RLadiesLocation* format (thankfully they are very compliant with this!). I use the `rtweet::search_users` function, setting the query to be searched with `q = 'RLadies'` and the number of users to retrieve with `n = 1000`, that being the maximum from a single search. As I want a dataframe as a result, I set the `parse` parameter to `TRUE`. This way I get 1,000 rows of users, with 36 variables regarding them. I'm only showing the variables I'm going to use, but there is a lot of extra information there. 

```{r load_data, echo = FALSE, message = FALSE, warning = FALSE}
# The data isn't in this repository, you can find everything I use here:
# https://github.com/d4tagirl/R-Ladies-growth-maps

library(knitr)
knitr::opts_chunk$set(dpi = 130, fig.align = 'center', screenshot.force = FALSE, fig.cap = "")
options(width = 80, dplyr.width = 150)

library(readr)
library(dplyr)

url_csv <- 'https://raw.githubusercontent.com/d4tagirl/R-Ladies-growth-maps/master/users.csv'
users <- read_csv(url(url_csv)) %>% 
  select(-1)

```

```{r  false_load_data, eval = FALSE}
library(rtweet)

users <- search_users(q = 'RLadies',
                      n = 1000,
                      parse = TRUE)
```

Let's see what it returns:

```{r message = FALSE, warning = FALSE}
library(DT)
datatable(users[, c(2:5)], rownames = FALSE,
          options = list(pageLength = 5))
```

<br/>
As I get so many duplicate users (nearly half of them!), I suspect it retrieves the user if `q` matches the user's _description_, _name_ or _screen\_name_ (handle), but also if it matches something they tweeted (neither the [Twitter API documentation](https://dev.twitter.com/rest/reference/get/users/search) nor the [`rtweet::search_users` one](https://cran.r-project.org/web/packages/rtweet/rtweet.pdf) are clear about this).

I used `DT::datatable` just in case someone wants to go through whats on the whole table (of course I'm thinking about the R-Ladies community here `r emo::ji("heart_eyes")` ). It was not easy to set up the environment for my blog to show this table (it uses the `htmlwidgets` package), but luckily my hubby was more than willing to help me with that part `r emo::ji("sweat_smile")` If you are using RStudio it is just as simple as installing the `DT` package, or you can always use `knitr::kable(head(users[, c(2:5)]), format = "html")` to see the first rows.

## Cleaning the data

First I remove all the duplicates, and then I keep only the handles that comply with the stipulated format, using a regular expression. I filter out 3 additional handles: 
- _'RLadies'_, whose _name_ is _'Royal Ladies'_ and I assume has something to do with royalty by the crown on their profile picture `r emo::ji("princess")` 
- _'RLadies\_LF'_, a Japanese account that translated as follows on _Google Translator_: _'Rakuten Ichiba fashion delivery'_.
- _'RLadiesGlobal'_, because it is not a chapter, so I don't want to include it on the plot. 

Then I format the _date class_ variable `created_at` as `%Y-%m-%d` (just because seeing the hours, minutes and seconds annoys me!), generate the age in days `age_days` and select the variables I will use for my analysis.

```{r  head_rladies, message = FALSE, warning = FALSE}
library(dplyr)
library(lubridate)
library(stringr)
library(tidyr)

rladies <- unique(users) %>%
  filter(str_detect(screen_name, '^(RLadies).*') & 
           !screen_name %in% c('RLadies', 'RLadies_LF', 'RLadiesGlobal')) %>% 
  mutate(created_at = format(as.Date(created_at), format = '%Y-%m-%d'),
         age_days = difftime(as.Date('2017-5-15'), created_at, unit = 'days')) %>%
  select(screen_name, location, created_at, followers = followers_count, age_days)
```

One final fix: I have some missing values on `location` that I'll need for geocoding the chapters, so I use an auxiliary table `lookup` to match the `screen_name` with the `location`, using `dplyr::left_join`. 

```{r , message = FALSE, warning = FALSE}
library(tibble)
lookup <- tibble(screen_name = c('RLadiesLx', 'RLadiesMTL' , 'RLadiesSeattle'), 
                 location    = c('Lisbon'   , 'Montreal'   , 'Seattle'      ))

rladies <- rladies %>%
  left_join(lookup, by = 'screen_name') %>%
  mutate(location = coalesce(location.x, location.y)) %>%
  select(-location.x, -location.y)
```

There are two additional chapters with no presence on Twitter: one in _Taipei, Taiwan_, and the other in _Warsaw, Poland_. I add them according to their creation date, using the number of members on their Meetup account as followers.

```{r  message = FALSE, warning = FALSE}
rladies <- rladies %>% 
  add_row(      
    screen_name = 'RLadiesTaipei',
    location = 'Taipei',
    created_at = as.Date('2014-11-15'),
    followers = 347) %>% 
  add_row(      
    screen_name = 'RLadiesWarsaw',
    location = 'Warsaw',
    created_at = as.Date('2016-11-15'),
    followers = 80)

datatable(rladies, rownames = FALSE,
          options = list(pageLength = 5))
```

<br />
As my ultimate goal is to plot the chapters on a map, I need to obtain the _latitude_ and _longitude_ for each one of them. That's when the `ggmap` package really comes in handy: it interacts with _Google Maps_ to retrieve the coordinates from the _location_, and I don't even have to worry about getting it into a specific format, because it is so good that it doesn't need it! (my first try was actually by extracting the cities using regular expressions, but many of the chapters didn't match or matched wrongly, so I tried it this way and it worked perfectly!)

Since the `ggmap::geocode` function returns 2 columns, I thought about calling it twice: once for the longitude and once for the latitude. But I didn't like it because it was awfully inefficient, and the geocoding takes some (really long!) time. It was going to be something like this:

```{r  false2, eval = FALSE}
library(ggmap)

rladies <- rladies %>% 
  mutate(lon = geocode(location)[,1],
         lat = geocode(location)[,2])
```

Doing some research (and benefitting from [Amelia](http://www.amelia.mn/)'s super helpful suggestion!) I finally decided to use the `purrr::map` function for capturing both values in a single column of the dataframe, and then transform it into two separate columns with `tidyr::unnest`. All of this with never having to leave the `tidyverse` world `r emo::ji("smirk")`

I'm doing it in two steps to see the intermediate result, with the two columns in a single variable of the dataframe.

```{r false3, eval = FALSE}
library(ggmap)
library(purrr)

rladies <- rladies %>% 
  mutate(longlat = purrr::map(.$location, geocode)) 
```

```{r echo = FALSE, message = FALSE, warning = FALSE, screenshot.force = FALSE}
rladies <- readRDS(gzcon(url('https://github.com/d4tagirl/R-Ladies-growth-maps/raw/master/rladies_longlat.rds'))) %>% 
  mutate(created_at = as.character(created_at))

kable(head(rladies))
```

```{r}
rladies <- rladies %>% 
  unnest() 
```

```{r echo = FALSE}
datatable(rladies, rownames = FALSE,
          options = list(pageLength = 5))        
```

<br />
That's it! Now the dataframe is ready for me to use it for visualizing these Twitter users on a map (considering their sizes and dates of creation), and make some interactive maps and animations! 

If you enjoyed this article, check out [the next one of the series here]({% post_url 2017-04-26-how-to-deal-with-ggplotly-huge-maps %}) or [the code in my GitHub repo](https://github.com/d4tagirl/R-Ladies-growth-maps). You are also welcome to leave your comments and suggestions below or [mention me on Twitter](https://twitter.com/intent/tweet?user_id=114258616). Thank you for reading `r emo::ji("wink")`
