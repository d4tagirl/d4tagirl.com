---
layout: post
title:  'Scrapeando las Sesiones Parlamentarias de Uruguay'
date: "2018-04-03 17:11:29 UYT"
published: true
tags: [rstats, open data, scraping, rvest, pdftools, parlamento, uruguay, senadores, diputados, es, r]
description: "Scraping de las sesiones de diputados y senadores desde 2017 hasta hoy y extracción de los textos de los archivos en formato pdf."
---
  
En Uruguay venimos avanzando con las iniciativas de datos abiertos, pero aún queda mucho camino por recorrer. Uno de los impedimentos para analizar datos es que no siempre no son fácilmente consumibles, sea porque no hay una forma sistemática de descargarlos o porque están en formato pdf, que no es muy amigable para ser interpretado por máquinas. En este artículo muestro cómo se pueden sortear ambas dificultades usando los paquetes [rvest](https://github.com/hadley/rvest) y [pdftools](https://github.com/ropensci/pdftools) respectivamente, y tener los Diarios de Sesiones descargados en el mejor formato posible para analizarlos. 

<!--more-->
  
```{r setup, echo = FALSE}
# you can find everything I use here:
# https://github.com/d4tagirl/uruguayan_parliamentary_session_diary
library(emo)
library(knitr)
knitr::opts_chunk$set(fig.align = 'center', screenshot.force = FALSE, fig.cap = "",
                      dpi = 120, message = FALSE, warning = FALSE)
options(width = 80, dplyr.width = 150)
```

```{r load_rds_diputados_senadores, echo = FALSE}
url_rds_diputados <- 'https://github.com/d4tagirl/uruguayan_parliamentary_session_diary/raw/master/data/pdf_diputados'
diputados <- readRDS(url(url_rds_diputados)) 

url_rds_senadores <- 'https://github.com/d4tagirl/uruguayan_parliamentary_session_diary/raw/master/data/pdf_senadores'
senadores <- readRDS(url(url_rds_senadores)) 
```

En Uruguay venimos avanzando con las iniciativas de datos abiertos, pero aún queda mucho camino por recorrer. Algunas de las dificultades para analizar datos abiertos son: 

  * que no siempre no es fácil acceder a ellos de forma sistemática: 
  por ejemplo porque no están todos juntos en un archivo comprimido para descargarlos, o no existe una API para acceder a la información,
    
  * que están en formato pdf del que no es tan fácil extraer información como de un archivo txt o csv, por nombrar algunos formatos. 
  
En particular hay dos problemas que quiero resolver: 
  
  * Descargar los archivos en formato pdf de las Sesiones Parlamentarias de Diputados y Senadores de forma sistemática, haciendo [lo que se conoce como *web scraping*](https://es.wikipedia.org/wiki/Web_scraping).

  * Extraer el texto contenido en los archivos en formato pdf.

En este artículo muestro cómo se pueden sortear ambas dificultades, usando los paquetes [rvest](https://github.com/hadley/rvest) para explorar la web y descargar los Diarios de Sesiones, y [pdftools](https://github.com/ropensci/pdftools) para extraer el contenido de los archivos en formato pdf.

# Está permitido que un robot se comunique con estas páginas?

Si bien se trata de datos abiertos, hay ciertas normas *de etiqueta* que es recomendable seguir. Respetar las reglas que establecieron los que administran el sitio acerca de cómo quieren que la gente lo use es una de ellas. Puede ser que sólo quieran que se navegue *a mano*, entonces no debería intentar acceder de la forma en que estoy planeando.

Para ver si la sección del sitio web que quiero navegar permite ser accedida por un *robot* (que es lo que pretendo construir! `r emo::ji("robot")`), examino [el archivo *robots.txt*](https://en.wikipedia.org/wiki/Robots_exclusion_standard), donde se establece para cada sección del sitio si este uso es adecuado. Desde R se puede hacer fácilmente usando [el parquete robotstxt](https://github.com/ropenscilabs/robotstxt) de [rOpenSci](https://ropensci.org/), que [Maëlle mencionó en su post donde scrapea datos de *The Guardian*](http://www.masalmon.eu/2017/10/02/guardian-experience/).

Este es todo el archivo *robots.txt* del [sitio web del Parlamento](https://parlamento.gub.uy):
  
```{r robotstxt_all}
robotstxt::get_robotstxt("https://parlamento.gub.uy")
```

[Lo que no aparece como *"Disallow"* es permitido por defecto](https://stackoverflow.com/a/40186203/7248543), y la url a la que quiero acceder usando un robot es [https://parlamento.gub.uy/documentosyleyes/documentos/diarios-de-sesion](https://parlamento.gub.uy/documentosyleyes/documentos/diarios-de-sesion). No dice en ningún lugar del documento `Disallow: /documentosyleyes/`, entonces está permitido!

Pero por las dudas, chequeo esa url en particular:
  
```{r robotstxt_sesiones}
robotstxt::paths_allowed("https://parlamento.gub.uy/documentosyleyes/documentos/diarios-de-sesion")
```

Tengo luz verde para scrapear la web `r emo::ji("raised_hands")`.

# Url de las páginas que quiero *scrapear*

Mi intención es descargar las sesiones de Diputados y Senadores desde el 1º/1/2017 hasta el 31/3/2018. Al incorporar estos filtros manualmente en la página [https://parlamento.gub.uy/documentosyleyes/documentos/diarios-de-sesion](https://parlamento.gub.uy/documentosyleyes/documentos/diarios-de-sesion), la url se va modificando para incorporar esta información. Eso hace las cosas algo menos complicadas para mi, porque una vez que me doy cuenta cómo se comporta la url con esos filtros, puedo generar la url nueva y directamente acceder a ella. Así construyo la nueva url que se compone de la url original, algo de texto adicional y los filtros de fechas, de la siguiente forma:

```{r url_diputados}
date_init <- "01-01-2017"
date_end  <- "31-03-2018"

url_diputados <- paste0("https://parlamento.gub.uy/documentosyleyes/documentos/diarios-de-sesion?Cpo_Codigo_2=D&Lgl_Nro=48&DS_Fecha%5Bmin%5D%5Bdate%5D=",
                        date_init,
                        "&DS_Fecha%5Bmax%5D%5Bdate%5D=",
                        date_end,
                        "&Ssn_Nro=&TS_Diario=&tipoBusqueda=T&Texto=")
url_diputados
```

```{r url_senadores}
url_senadores <- paste0("https://parlamento.gub.uy/documentosyleyes/documentos/diarios-de-sesion?Cpo_Codigo_2=S&Lgl_Nro=48&DS_Fecha%5Bmin%5D%5Bdate%5D=",
                        date_init,
                        "&DS_Fecha%5Bmax%5D%5Bdate%5D=",
                        date_end,
                        "&Ssn_Nro=&TS_Diario=&tipoBusqueda=T&Texto=")

url_senadores
```

# Y ahora? Cómo selecciono el contenido de la página? 

Las páginas web son archivos html que el navegador interpreta y los transforma en lo que nosotros vemos. No voy a entrar en muchos detalles de cómo interpretar un archivo html (porque recién estoy aprendiendo!), pero acá describo la  forma más intuitiva que encontré para seleccionar el contenido del html al que quiero acceder. 

Como se muestra en la animación a continuación, usando el [Selector Gadget](http://selectorgadget.com/) (que tiene una extensión para Google Chrome muy conveniente) me paro con el mouse sobre uno de los links a los pdfs y hago click. Ahí queda pintada toda la columna, porque pinta todos los elementos que son de la misma *clase* (no nos preocupemos de qué es una *clase* ahora). Lo importante es que necesito *el nombre de la clase* para lo que viene a continuación, entonces copio el texto que aparece en el recuadro (en este caso es `.views-field-DS-File-IMG`). 

![](/figure/source/scrapeando-las-sesiones-parlamentarias-de-uruguay/2018-04-03-scrapeando-las-sesiones-parlamentarias-de-uruguay/selector_gadget.gif) 

# Web scraping!

## Extraigo los pdfs

Ahora es que empiezo a usar el paquete [rvest](https://github.com/hadley/rvest). Defino una función que descarga los pdfs y los guarda en un dataframe, haciendo algunas transformaciones. Para explicar lo que hace la función voy a ignorar que como las sesiones son muchas, las muestra en dos páginas separadas. Es verdad que podría haber puesto un poco más de esfuerzo en hacer esta función generalizable a *n* páginas, pero como sabía que tenía sólo 2, lo dejé así `r emo::ji("innocent")`

```{r funcion_download_pdf}
library(dplyr)
library(rvest)
library(purrr)
library(tibble)
library(pdftools)

extract_pdf <- function(url, pag = 1) {
  
  if (pag == 2) {
    url <- url %>%
      read_html() %>%
      html_nodes(".pager-item a") %>%
      html_attr("href") %>%
      map(~ paste0("https://parlamento.gub.uy", .)) %>%
      unlist()
    }
  
  pdfs <- url %>%
    ead_html() %>%
    html_nodes(".views-field-DS-File-IMG a") %>%   # seleccionar clase
    html_attr("href") %>%
    map(~ paste0("https://parlamento.gub.uy", .)) %>%
    map(~ paste0(pdf_text(.), collapse = ' ')) %>%
    map(~ stri_trans_general(tolower(.), id = "latin-ascii")) %>%
    map(~ stri_replace_all(., replacement = "", regex = "\\\n")) %>% 
    map_df(function(pdf) {tibble(pdf)})
  
  return(pdfs)
}
```

La primera parte de la función es la que voy a ignorar, donde lo que hago es modificar la url para indicar que quiero ir a la segunda página (se puede aplicar la misma lógica que la que voy a usar a continuación para interpretar esta parte del código).

Lo interesante pasa cuando empiezo a procesar la url: `read_html()` "lee" el contenido de la página, para que pueda buscar lo que me interesa. Mi objetivo es encontrar todos los archivos pdf (por eso me interesaba conocer *el nombre de la clase* de esos elementos, que descubrí antes). Con `html_nodes()` voy extrayendo los *nodos* (tampoco nos preocupemos ahora por saber qué son exactamente), y en este caso el *nodo* que me interesa es `.views-field-DS-File-IMG a`: 

  * tiene un punto adelante para indicar que se trata de elementos de una clase, seguido del nombre de la clase `views-field-DS-File-IMG` (lo que copiamos con el *Selector Gadget*),

  * `a` es la etiqueta que html usa para definir elementos que son un hipervínculo. 

Ahora puedo seleccionar el atributo `href`, que es el link al pdf, con la función `html_attr()`. Acá hay que tener cuidado porque la ruta es relativa (es decir que falta poner "https://parlamento.gub.uy" antes para tener la ruta completa).

Al aplicar la función hasta acá, lo que tengo es una lista con una url por cada pdf que aparece en la página. Por eso luego uso la función `purrr::map()` para aplicar a cada elemento de la lista (cada link a los pdfs), una función. Las transformaciones que aplico a cada elemento de la lista son, en forma sucesiva, las siguientes:

  * `paste0("https://parlamento.gub.uy", .)` completo la ruta absoluta, pegando "https://parlamento.gub.uy" adelante del link relativo,

  * `paste0(pdf_text(.), collapse = ' ')` usando el paquete `pdftools` extraigo la información del pdf con la función `pdf_text()` y colapso todas las páginas en un mismo string,

  * `stri_trans_general(tolower(.), id = "latin-ascii")` saco los caracteres especiales y dejo todo en minúscula,
  
  * `stri_replace_all(., replacement = "", regex = "\\\n")` elimino los saltos de línea, 

  * `tibble(pdf)` transformo la lista en un dataframe.

Entonces ahora extraigo los pdfs de las dos páginas y los junto en un único dataframe (voy a mostrar el proceso para Diputados, pero es análogo para Senadores).

```{r false_extract_pdf_diputados, eval=FALSE}
pdf_diputados_pag1 <- extract_pdf(url_diputados, pag = 1)
pdf_diputados_pag2 <- extract_pdf(url_diputados, pag = 2)

pdf_diputados <- bind_rows(pdf_diputados_pag1, pdf_diputados_pag2)

library(knitr)
library(kableExtra)

knitr::kable(head(pdf_diputados) %>%
               select(pdf) %>% 
               mutate(pdf = substr(pdf, start=1, stop=500)),
             format = "html") 
```

```{r print_pdf_diputados, echo=FALSE}
library(knitr)
library(kableExtra)

knitr::kable(head(diputados) %>% 
               select(pdf) %>% 
               mutate(pdf = substr(pdf, start=1, stop=500)), 
             format = "html") 
```

### `r emo::ji("warning")` Advertencia `r emo::ji("warning")`

La función `pdftools::read_pdf()` lee los renglones de izquierda a derecha. En los Diarios de Sesiones hay algunas páginas que se organizan con texto en dos columnas, entonces hay renglones que, leídos de esa forma, quedan incoherentes. Esto hay que tenerlo en cuenta para ver si el tipo de análisis que quiero hacer tiene sentido o no. Por ejemplo, si lo que quiero es analizar [*n-gramas*](https://es.wikipedia.org/wiki/N-grama) donde el orden de las palabras es importante, voy a tener problemas porque estaría considerando palabras de distintas columnas de texto, como si vinieran una a continuación de la otra. Para analizar sentimiento con [*bolsa de palabras (bag of words)*](https://es.wikipedia.org/wiki/Modelo_bolsa_de_palabras) no hay problema, porque el orden de las palabras no es relevante.

## Extraigo fecha y número de sesión

Hago una segunda función para extraer otros la fecha y el número de la sesión, porque se puede dar el caso de tener más de una sesión en la misma fecha.

```{r funcion_download_metadata}
extract_metadata <- function(url, info, pag = 1){
  if (info == "fecha") nodes = "td.views-field-DS-Fecha"
  if (info == "sesion") nodes = "td.views-field-Ssn-Nro"
  if (pag == 2){
    url <- url %>%
      read_html() %>%
      html_nodes(".pager-item a") %>%
      html_attr("href") %>%
      map(~ paste0("https://parlamento.gub.uy", .)) %>%
      unlist() 
    }
  
  url %>% 
    read_html() %>% 
    html_nodes(nodes) %>% 
    html_text() %>% 
    map(~str_extract(., "[0-9\\-]+")) %>%  # esta expresión regular matchea tanto la fecha como el número de sesión
    unlist()
}
```

Extraigo la fecha y la sesión.

```{r false_extract_metadata_diputados, eval=FALSE}
# extraigo fechas
pdf_fechas_diputados_pag1 <- extract_metadata(url_diputados, info = "fecha", pag = 1)
pdf_fechas_diputados_pag2 <- extract_metadata(url_diputados, info = "fecha", pag = 2)

# junto todos las fechas y las convierto en un df
pdf_fechas_diputados <- c(pdf_fechas_diputados_pag1, pdf_fechas_diputados_pag2) %>% 
  tbl_df() %>% 
  transmute(fecha = as.Date(value, "%d-%m-%Y"))

# extraigo sesiones
pdf_sesion_diputados_pag1 <- extract_metadata(url_diputados, info = "sesion", pag = 1)
pdf_sesion_diputados_pag2 <- extract_metadata(url_diputados, info = "sesion", pag = 2)

# junto todos las sesiones y las convierto en un df
pdf_sesion_diputados <- c(pdf_sesion_diputados_pag1, pdf_sesion_diputados_pag2) %>% 
  tbl_df() %>% 
  rename(sesion = value)

knitr::kable(head(pdf_sesion_diputados) %>% select(fecha),
             format = "html") 
```

```{r print_fecha_diputados, echo=FALSE}
knitr::kable(head(diputados) %>% select(fecha), 
             format = "html") 
```

```{r false_print_sesion_diputados, eval = FALSE}

knitr::kable(head(pdf_sesion_diputados) %>% select(sesion),
             format = "html") 
```

```{r print_sesion_diputados, echo=FALSE}
knitr::kable(head(diputados) %>% select(sesion),
             format = "html") 
```
Juntando todo, armo el dataframe de Diputados con la fecha, la sesión y el texto del pdf.

```{r false_diputados_merge, eval=FALSE}
diputados <- bind_cols(pdf_fechas_diputados, pdf_sesion_diputados, pdf_diputados) %>% 
  unite("fecha_sesion", c(fecha, sesion), remove = FALSE) %>%
  distinct() # la primer sesión de la segunda página es igual a la última sesión de la primera página

```

Con este dataframe es con el que voy a trabajar para los Diputados, que tiene `r nrow(diputados)` sesiones.

```{r false_diputados_final, eval = FALSE}
knitr::kable(head(diputados) %>% 
               mutate(pdf = substr(pdf, start=1, stop=500)), 
             format = "html") 
```

```{r diputados_final, echo = FALSE}
knitr::kable(head(diputados) %>% 
               mutate(pdf = substr(pdf, start=1, stop=500)), 
             format = "html") 
```

Y con este dataframe es con el que voy a trabajar para los Senadores, que tiene `r nrow(senadores)` sesiones.

```{r false_senadores_final, eval = FALSE}
knitr::kable(head(senadores) %>% 
               mutate(pdf = substr(pdf, start=1, stop=500)), 
             format = "html") 
```

```{r senadores_final, echo = FALSE}
library(stringi)

# Acá saco los saltos de línea. Como probé algunas otras cosas, me servía que estuvieran, pero en el post no incluyo nada de eso.
diputados <- diputados %>%
  mutate(pdf = stri_replace_all(pdf, replacement = "", regex = "\\\n"))

senadores <- senadores %>%
  mutate(pdf = stri_replace_all(pdf, replacement = "", regex = "\\\n"))

knitr::kable(head(senadores) %>% 
               mutate(pdf = substr(pdf, start=1, stop=500)), 
             format = "html") 
```

# Tadá `r emo::ji("tada")`

Ahora con los datos en este formato, estoy en condiciones de analizar las sesiones!

Para más información acerca de cómo trabajar con datos de la web, hay un [tutorial de Arvid Kingl en Datacamp en inglés para usar `rvest`](https://www.datacamp.com/community/tutorials/r-web-scraping-rvest) y hay un [curso de Charlotte Wickham y Oliver Keyes, también en inglés y en Datacamp,](https://www.datacamp.com/courses/working-with-web-data-in-r) que habla además de otras formas de consumir datos de la web, como a través de APIs.

Todo lo que usé en este artículo (y más!) está [disponible en GitHub](https://github.com/d4tagirl/uruguayan_parliamentary_session_diary). Espero que haya resultado útil! Dejame tus comentarios abajo o [mencioname en Twitter](https://twitter.com/intent/tweet?user_id=114258616) `r emo::ji("smiley")`
